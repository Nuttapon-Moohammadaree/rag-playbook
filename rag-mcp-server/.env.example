# RAG MCP Server Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# Required: LiteLLM API Configuration
# =============================================================================
LITELLM_API_KEY=your-api-key-here
LITELLM_BASE_URL=http://localhost:4000/v1

# =============================================================================
# Embedding Model
# =============================================================================
EMBEDDING_MODEL=BAAI/bge-m3
VECTOR_SIZE=1024

# =============================================================================
# Qdrant Vector Database
# =============================================================================
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=rag_documents

# =============================================================================
# SQLite Metadata Storage
# =============================================================================
SQLITE_PATH=./data/sqlite/rag.db

# =============================================================================
# Chunking Configuration
# =============================================================================
CHUNK_SIZE=512
CHUNK_OVERLAP=50
MIN_CHUNK_SIZE=100

# =============================================================================
# Search Configuration
# =============================================================================
SEARCH_LIMIT=10
SEARCH_THRESHOLD=0.5

# =============================================================================
# LiteLLM Timeout (milliseconds)
# =============================================================================
LITELLM_TIMEOUT=30000

# =============================================================================
# Reranking Configuration (BGE-reranker-v2-m3)
# =============================================================================
RERANKING_ENABLED=true
RERANKER_MODEL=BAAI/bge-reranker-v2-m3
RERANK_TOP_N=5
RERANK_CANDIDATES=4

# =============================================================================
# LLM Configuration
# =============================================================================
LLM_MODEL=gpt-oss-120b
QUERY_EXPANSION=true
AUTO_SUMMARY=false
AUTO_TAGS=false
HYDE_ENABLED=false

# =============================================================================
# Verification Pipeline Configuration
# =============================================================================
VERIFICATION_ENABLED=false
VERIFICATION_RELEVANCE_THRESHOLD=0.6
VERIFICATION_GROUNDING_THRESHOLD=0.7
VERIFICATION_MAX_PARALLEL=3
VERIFICATION_CACHE=true
VERIFICATION_CACHE_TTL_MS=300000

# =============================================================================
# REST API Configuration (for api/ server)
# =============================================================================
API_PORT=3001
API_HOST=0.0.0.0
